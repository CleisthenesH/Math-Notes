% Copyright 2022 Kieran W Harvie. All rights reserved.
\documentclass[12pt]{report}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amssymb}
 
\usepackage[OT2,T1]{fontenc}

\title{Fourier Notes}
\date{Copyright \textcopyright  \today. All Rights Reserved.}
\author{Kieran Harvie}

\DeclareSymbolFont{cyrletters}{OT2}{wncyr}{m}{n}

\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\Ai}{Ai}
\DeclareMathSymbol{\Sha}{\mathalpha}{cyrletters}{"58}
\DeclareMathOperator{\Hom}{Hom}

\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}
In the simplest terms the Fourier Transform is a method of representing a function as a weighted integral of complex exponentials.
A good basis for the theory of this transform is the representation of a single function as a weighted integral of complex exponentials, the delta function.

With this representation of the delta function so many of the transforms properties, like convulsion, inverse, and shifting, will become obvious.
The hope is that this obviousness will help the transforms' wielder not only apply the Fourier transform but avoid large technical errors by making them stand out more when they are only small.

\section{The Delta Function}
We may characterize the delta function by two of its properties
\begin{enumerate}
\item Zero at non-zero values, $x\neq 0 \rightarrow \delta(x) = 0$.
\item Normalized on the real line, $\int_{-\infty}^{\infty}\delta(x) = 1$.
\end{enumerate}

We wish to represent such a function as an integral of weighted complex exponentials and we might expect something like this:

\[\delta(x) = \int_{-\infty}^{\infty}a\exp(-bixt)\,dt\]

Since when $x\neq 0$ over an infinite range all the phases cancel themselves out.
But when $x=0$ the integrand is constant meaning the integral infinite, enabling the non-zero normalization.
But to get values for $a,b$ we need to be more precise about what we mean by `infinite' and rigorous with our method.
\\

To this end we will approximate the delta function as the limit of normalized Gaussian as the standard deviation approaches $0$:
\[\delta(x) = \lim_{\sigma\rightarrow\infty}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2}\left(\frac{x}{\sigma}\right)^2\right)\]
To this approximation consider the following integral:
\begin{equation*}
\begin{aligned}
	&\int_{-\infty}^{\infty}\exp(-\alpha t^2) \exp(-ixt)\,dt \\
	=&\int_{-\infty}^{\infty}\exp\left(-\alpha\left(t+\frac{ix}{2\alpha}\right)^2-\frac{x^2}{4\alpha}\right)\,dt\\
	=& \sqrt{\frac{\pi}{\alpha}}\exp\left(-\frac{x^2}{4\alpha}\right)
\end{aligned}
\end{equation*}
The outside integral looks close to a Gaussian.
First we make the Gaussian outside the integral a standard form, start with $\alpha = \sigma^2/2$:
\[\frac{1}{\sigma}\sqrt{2\pi}\exp\left(-\frac{1}{2}\left(\frac{x}{\sigma}\right)^2\right) = \int_{-\infty}^{\infty}\exp\left(-\frac{\sigma^2t^2}{2}\right)\exp(-ixt)\,dt\]
Normalize the outside Gaussian by multiplying by $(2\pi)^{-1}$:
\[\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{x}{\sigma}\right)^2\right) = \int_{-\infty}^{\infty}\frac{1}{2\pi }\exp\left(-\frac{\sigma^2t^2}{2}\right)\exp(-ixt)\,dt\]
Taking the limit $\sigma \rightarrow 0$ and the integrand becomes:
\[\lim_{\sigma \rightarrow 0}\frac{1}{2\pi }\exp\left(-\frac{\sigma^2t^2}{2}\right)  = \frac{1}{2\pi} \]
Hence we obtain:
\[\delta(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty}\exp(-ixt)\,dt\]
And trivially:
\[\delta(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty}\exp(ixt)\,dt\]

\section{The Sampling Property}
The two defining properties of the delta function combine to the following important result.
If $f$ is continuous at $0$ then:
\[\int_{-\infty}^{\infty}f(x)\delta(x)\,dx = f(0)\]

Since $f$ is continuous at $0$ for all $\epsilon > 0$ we can find $\delta > 0$ such that:
\[x \in (-\delta,\delta) \Rightarrow |f(x)-f(0)| < \epsilon\]
Then:
\begin{equation*}
\begin{aligned}
	&\left|\int_{-\infty}^{\infty}f(x)\delta(x)\,dx-f(0)\right| \\
	=&\left|\int_{-\delta}^{\delta}f(x)\delta(x)\,dx-f(0)\right| \\
	=&\left|\int_{-\delta}^{\delta}f(x)\delta(x)\,dx-\int_{-\delta}^{\delta}f(0)\delta(x)\,dx\right| \\
	=&\left|\int_{-\delta}^{\delta}(f(x)-f(0))\delta(x)\,dx\right| \\
	\leq&\int_{-\delta}^{\delta}|f(x)-f(0)|\delta(x)\,dx \\
	\leq&\int_{-\delta}^{\delta}\epsilon\delta(x)\,dx \\
	=&\epsilon \\
\end{aligned}
\end{equation*}
This being true for all $\epsilon>0$ requires:
	\[\int_{-\infty}^{\infty}f(x)\delta(x)\,dx = f(0)\]
Completing the proof.
\\

As you likely guessed this property is called "sampling" since the integral samples the value $f(0)$ from $f$.
This sampling property will act as the final steps of multiple proofs where we get a value from an integral by creating a delta function within it.

\section{The Fourier Transform and it's Inverse}
With the sampling property in hand we will now finish the introduction by defining the Fourier Transform and it's Inverse.
Consider a given function $f$ define it's Fourier transform $\hat{f}$ as:
\[\hat{f}(k) = \int_{-\infty}^{\infty}f(x)\exp(-ikx)\,dx\]
\\

What is remarkable about $\hat{f}$ is that $f$ can be recovered from it:
\label{sec:inv-trans}
\begin{equation*}
\begin{aligned}
	\frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{f}(k)\exp(ixk)\,dk 
	=& \frac{1}{2\pi}\int_{-\infty}^{\infty}\left(\int_{-\infty}^{\infty}f(t)\exp(-itk)\,dt\right)\exp(ixk)\,dk \\
	=& \int_{-\infty}^{\infty}f(t)\left(\frac{1}{2\pi}\int_{-\infty}^{\infty}\exp(ik(x-t))\,dk\right)\,dt \\
	=& \int_{-\infty}^{\infty}f(t)\delta(x-t),dt \\
	=& f(x) \\
\end{aligned}
\end{equation*}

And looking at the final equality by itself:
\[f(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{f}(k)\exp(ikx)\,dk\]
We see that $\hat{f}$ gives a representation of $f$ in terms of a weighted integral of complex exponentials, as desired.

\chapter{Fundamentals}
Before moving towards applications of the Fourier transform it will be useful to collect some basic properties, theorems and symmetries.
As advertised in the introduction most of these will follow from the Fourier transform of the delta function.
\section{Basic Properties}
\subsection{Linearity}
It's almost so trivial as not need to be said, but the Fourier transform is linear,
Let:
\[h(x)=af(x)+bg(x)\]
Then:
\begin{equation*}
\begin{aligned}
\hat{h}(k) =& \int_{-\infty}^{\infty}h(x)\exp(-ikx)\,dx \\
=& \int_{-\infty}^{\infty}(af(x)+bg(x))\exp(-ikx)\,dx \\
=& a\int_{-\infty}^{\infty}f(x)\exp(-ikx)\,dx +b\int_{-\infty}^{\infty}g(x)\exp(-ikx)\,dx \\
=& a\hat{f}(k)+b\hat{g}(k) \\
\end{aligned}
\end{equation*}

\subsection{Duality}
Although other properties are more useful and obvious but learning about the duality first lets us double all coming properties.
Let:
\[g(x) = \hat{f}(x)\]
Then:
\begin{equation*}
\begin{aligned}
\hat{g}(k) =&\int_{-\infty}^{\infty}g(x)\exp(-ikx)\,dx \\
=&\int_{-\infty}^{\infty}\hat{f}(x)\exp(-ikx)\,dx \\
=& \int_{-\infty}^{\infty}\left(\int_{-\infty}^{\infty}f(t)\exp(-itx)\right)\,dt\exp(-ikx)\,dx \\
=& 2\pi\int_{-\infty}^{\infty} f(t) \left(\frac{1}{2\pi} \int_{-\infty}^{\infty} \exp(-ix(t+k))\,dx\right)\,dt \\
=& 2\pi\int_{-\infty}^{\infty} f(t) \delta(t+k)\,dt \\
=& 2\pi f(-k)
\end{aligned}
\end{equation*}
You can also show this by change of variable $x\mapsto -x$ and then apply the definition of inverse transform.
I don't like this because I fear getting the order of operations right in a partial/total derivative way.

\subsection{Translation}
A simple but useful property that we get immediately from definitions.
Let:
\[g(x) = f(x+x_0)\]
Then:
\begin{equation*}
\begin{aligned}
\hat{g}(k) =& \int_{-\infty}^{\infty}g(x)\exp(-ikx)\,dx\\
=& \int_{-\infty}^{\infty}f(x+x_0)\exp(-ikx)\,dx\\
=&\exp(ikx_0) \int_{-\infty}^{\infty}f(x+x_0)\exp(-ik(x+x_0))\,dx\\
=&\exp(ikx_0) \hat{f}(k)\\
\end{aligned}
\end{equation*}

\subsection{Scaling}
Let:
\[g(x) = f(ax) \]
Then:
\begin{equation*}
\begin{aligned}
	\hat{g}(k) =& \int_{-\infty}^{\infty}f(ax)\exp(-ixk)\,dx\\
	=& \int_{-\infty}^{\infty}f(x)\exp\left(-ix\left(\frac{k}{a}\right)\right)\,\frac{dx}{|a|}\\
	=& \frac{1}{|a|}\hat{f}\left(\frac{k}{a}\right) \\
\end{aligned}
\end{equation*}

Note how the special case of $a=-1$ means the transform commutative with changing the sign of the argument.

\subsection{Differentiation} 
Let:
\[g(x) = f'(x)\]
Then:
\begin{equation*}
\begin{aligned}
	\hat{g}(k) =& \int_{-\infty}^{\infty}f'(x)\exp(-ixk)\,dx\\
	=& \left[f(x)\exp(-ixk)\right]_{-\infty}^{\infty}-\int_{-\infty}^{\infty}f(x)(-ik)\exp(-ixk)\,dx\\
	=&ik\int_{-\infty}^{\infty}f(x)\exp(-ixk)\,dx\\
	=& ik\hat{f}(k) \\
\end{aligned}
\end{equation*}

The vanishing term on the second line will be explained in full by duality and the Riemann-Lebesgue lemma.

\subsection{Conjugation}
Let:
\[g(x) = \overline{f(x)}\]
Then:
\begin{equation*}
\begin{aligned}
	\hat{g}(k) =& \int_{-\infty}^{\infty}\overline{f(x)}\exp(-ikx)\,dx\\
	=& \overline{\int_{-\infty}^{\infty}f(x)\exp(-i(-k)x)\,dx}\\
	=& \overline{\hat{f}(-k)} \\
\end{aligned}
\end{equation*}

\section{Theorems}
\subsection{Convolution Theorem}
An immediate application of the Shifting Property is the Convolution Theorem.
Let:
\[h(x) = \int_{-\infty}^{\infty}f(t)g(x-t)\,dt\]
Then:
\begin{equation*}
\begin{aligned}
\hat{h}(k) =& \int_{-\infty}^{\infty}h(x)\exp(-ikx)\,dx\\
=& \int_{-\infty}^{\infty}\left(\int_{-\infty}^{\infty}f(t)g(x-t)\,dt\right)\exp(-ikx)\,dx\\
=& \int_{-\infty}^{\infty}f(t)\left(\int_{-\infty}^{\infty}g(x-t)\exp(-ikx)\,dx\right)\,dt\\
=& \int_{-\infty}^{\infty}f(t)\exp(-itk)\hat{g}(k)\,dt\\
=& \hat{g}(k)\int_{-\infty}^{\infty}f(t)\exp(-itk)\,dt\\
=& \hat{g}(k)\hat{f}(k)\\
\end{aligned}
\end{equation*}

\subsection{Poisson Summation Formula}
There is an interesting summation formula that follows from the \hyperref[appx:dirac-comb]{Dirac Comb}:
\begin{equation*}
\begin{aligned}
	\sum_{n=-\infty}^{\infty}\hat{f}(Dn) = & \int_{-\infty}^{\infty}\hat{f}(k)\Sha_D(k)\,dk \\
	= &\frac{1}{D} \int_{-\infty}^{\infty}\hat{f}(k)\left(\sum_{n=-\infty}^{\infty}\exp\left(ik \frac{2\pi n}{D}\right)\right)\,dk\\
	= &\frac{1}{D} \sum_{n=-\infty}^{\infty}\int_{-\infty}^{\infty}\hat{f}(k)\exp\left(ik \frac{2\pi n}{D}\right)\,dk\\
	= &\frac{1}{D} \sum_{n=-\infty}^{\infty}f\left(\frac{2\pi n}{D}\right)\\
\end{aligned}
\end{equation*}

Can apparently be proved through Pontryagin Duality, which sounds cool. 

\subsection{Real, Imaginary, Odd, and Even Components}
Consider a function $f$ that is both \hyperref[sec:conjugation-sym]{purely real} and \hyperref[sec:parity-sym]{odd}.
Then by combining results from the symmetry section $\hat{f}$ is odd and purely imaginary.
\\

Similar arguments for the other combinations gives the following table:

\begin{center}
\begin{tabular}{c|c}
	$f$ & $\hat{f}$ \\ \hline
	Real and Even & Real and Even \\
	Real and Odd & Imaginary and Odd \\
	Imaginary and Even & Imaginary and Even \\
	Imaginary and Odd & Real and Odd \\
\end{tabular}
\end{center}

Interestingly these four groups are one-to-one.
Combining this table with the unique \hyperref[appx:real-img-odd-even]{representation of $f$} we know that the real and odd components of $\hat{f}$ depends solely on the Imaginary and odd components of $f$.

Basically splitting the Fourier transforms from one complex function on $\mathbb{R}$ to four real functions on $\mathbb{R}_+$.

\section{Symmetries}
\subsection{Parity Symmetry}
\label{sec:parity-sym}
Let $f$ be an \hyperref[appx:real-img-odd-even]{even or odd} function then $\hat{f}$ is the same:
\[f(x)\pm f(-x) = 0 \Rightarrow \hat{f}(x)\pm\hat{f}(-x) = 0\]

\subsection{Conjugation Symmetry}
\label{sec:conjugation-sym}
Let $f$ be a \hyperref[appx:real-img-odd-even]{purely real} function then $\hat{f}$ is the Hermitian:
\[f(x) - \overline{f(x)} = 0 \Rightarrow \hat{f}(x)+\overline{\hat{f}(-x)} = 0\]

To my knowledge there is no name for the \hyperref[appx:real-img-odd-even]{purely imaginary} but a similar relation applies:
\[f(x) + \overline{f(x)} = 0 \Rightarrow \hat{f}(x)-\overline{\hat{f}(-x)} = 0\]

\subsection{Translation Symmetry}
Let $f(x)$ be periodic with period $P$.
Define $f_k = \int_{0}^{P}f(x)\exp(-ikx)\,dx$ and consider:
\begin{equation*}
\begin{aligned}
	\hat{f}(k) =& \int_{-\infty}^{\infty}f(x)\exp(-ixk)\,dx \\
	=& \sum_{n=-\infty}^{\infty}\int_{nP}^{(n+1)P}f(x)\exp(-ixk)\,dx \\
	=& \sum_{n=-\infty}^{\infty}\int_{0}^{P}f(x)\exp(-ik(x+nP))\,dx \\
	=& \left(\int_{0}^{P}f(x)\exp(-ikx)\,dx\right)\sum_{n=-\infty}^{\infty}\exp(-iknP) \\
	=& \frac{2\pi}{P}\left(\int_{0}^{P}f(x)\exp(-ikx)\,dx\right)\sum_{n=-\infty}^{\infty}\delta\left(k-n\frac{2\pi}{P}\right)\\
\end{aligned}
\end{equation*}

This is an interesting result by itself, that when $f$ is periodic $\hat{f}$ is mostly zero except for where $k$ is a multiple of $\frac{2\pi}{P}$.
But if we substitute this expressing into the inverse transform we get a long but interesting result:
\begin{equation*}
\begin{aligned}
	f(x) =& \frac{1}{2\pi}\int_{-\infty}^{\infty}\left(\frac{2\pi}{P}\left(\int_{0}^{P}f(t)\exp(-ikt)\,dt\right)\sum_{n=-\infty}^{\infty}\delta\left(k-n\frac{2\pi}{P}\right)\right)\exp(ixk)\,dk\\
	      =&\frac{1}{P}\sum_{n=-\infty}^{\infty}\int_{-\infty}^{\infty}\delta\left(k-n\frac{2\pi}{P}\right)\left(\int_{0}^{P}f(t)\exp(-ikt)\,dt\right)\exp(ixk)\,dk \\
	      =&\frac{1}{P}\sum_{n=-\infty}^{\infty}\left(\int_{0}^{P}f(t)\exp(-itn\frac{2\pi}{P})\,dt\right)\exp\left(ixn\frac{2\pi}{P}\right)\,dk
\end{aligned}
\end{equation*}

\subsection{Radial Symmetry}
Start with our standard 2-D Fourier integral:
\[F(k_x,k_y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y)\exp(-i(k_x x + k_y y))\,dx\,dy\]
Assume we have radial symmetry such that:
\[f(r\cos(\theta),r\sin(\theta)) = g(r)\]
Make the following substitutions:
\begin{equation*}
\begin{aligned}
	x = r\cos(\theta),\quad k_x = k_r\cos(\phi) \\
	y = r\sin(\theta),\quad k_y = k_r\sin(\phi)\\
\end{aligned}
\end{equation*}
We get:
\begin{equation*}
\begin{aligned}
	&F(k_r\cos(\phi),k_r\sin(\phi)  \\
	=& \int_0^\infty\int_0^{2\pi}f(r\cos(\theta),r\sin(\theta))\exp(-ik_r r(\cos(\theta)\cos(\phi)+\sin(\theta)\sin(\phi)))r\,d\theta\,dr\\
	=& \int_0^\infty g(r)r\int_0^{2\pi}\exp(-ik_r r\cos(\theta-\phi))\,d\theta\,dr \\
\end{aligned}
\end{equation*}
Note that the inner integrand is a function of $\theta$ with period $2\pi$ which is the same as the integral bounds hence we can pick any $\phi$ in particular:
\[-\cos(\theta +\pi/2) = \sin(\theta)\]
Hence the total function has no dependence on $\phi$ giving:
\[F(k_r) = \int_0^\infty g(r)rJ_0(k_r r)\,dr\]

\chapter{Applications}


\section{Function Spaces}
Function spaces are sets of function between two fixed sets with with some additional structure, often naturally inherited from the fixed sets.
For our purposes the most important are $L^p(X)$ spaces which is the set of functions on the measurable space $X$ to $\mathbb{C}$ such that:
\[\|f\|_p = \left(\int_{X}|f|^p\right)^\frac{1}{p} < \infty\]
With addition and scalar multiplication defined naturally as:
\begin{equation*}
\begin{aligned}
(f+g)(x) =& f(x)+g(x) \\
(\lambda f)(x) =& \lambda f(x)
\end{aligned}
\end{equation*}

Fourier analysis can be used to study $L^1(\mathbb{R})$ and $L^2(\mathbb{R})$, and these spaces can be used to study the Fourier transform.
An overview is that $L^1(\mathbb{R})$ is a yell behaved space where step functions are dense and integration works how you would expect.
While $L^2(\mathbb{R})$ is a less well-behaved but Hilbert space the Fourier transform can be extended to.

\subsection{Plancherel Theorem}
\label{sec:plancherel}
Consider the following deviation:
\begin{equation*}
\begin{aligned}
&\int_{-\infty}^{\infty}\hat{f}(k)\overline{\hat{g}(k)}\,dk \\
=&\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x)\overline{g(y)}\exp(-ik(x-y))\,dxdy\,dk \\
=&\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x)\overline{g(y)}\int_{-\infty}^{\infty}\exp(-ik(x-y))\,dk\,dxdy \\
=&2\pi\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x)\overline{g(y)}\delta(x-y)\,dxdy \\
=&2\pi\int_{-\infty}^{\infty}f(x)\overline{g(x)}\,dx \\
\end{aligned}
\end{equation*}

The special case where $f = g$ is called the Plancherel theorem:
\begin{equation*}
\begin{aligned}
\int_{-\infty}^{\infty}|f(x)|^2\,dx = \frac{1}{2\pi}\int_{-\infty}^{\infty}|\hat{f}(k)|^2\,dk \\
\end{aligned}
\end{equation*}

At first glance this theorem seems only useful in showing that the square of the Fourier transform is the power spectrum of a signal and nothing more.
But this unassuming theorem is actually very important to the formal analysis of the Fourier transform.
Unlike in this text where we have been, and will continue to be, very handwavey when it comes to matters of changing the order of integration, delta functions, moving limits inside of integrals, etcetera. 

The Plancherel theorem can be formally proved for all members of $L^1(\mathbb{R})\cap L^2(\mathbb{R})$ and something called Plancherel transform can be defined which uniquely can extends the theorem to all of $L^2(\mathbb{R})$.
From the Plancherel theorem we can see the Plancherel transform, down to a scale factor, is an isometry of $L^2(\mathbb{R})$, a super useful property, and agrees with the Fourier transform when the function is also a member of $L^1(\mathbb{R})$.
From this extension we can casually speak of the Fourier transform of $L^2(\mathbb{R})$ function while formally talking about the Plancherel transform of the function while.

Don't feel to bad for Plancherel for doing all his formal work being seemingly attributed to Fourier.
This "Plancherel method" of understanding a transform by defining a similar isometry on a well behaved subset and then extending it to the wider space.
Is also used in understanding spherical functions and extending Fourier analysis to topological groups which aren't commutative where Plancherel name is much more common.
\\

As a final note, the Plancherel transform can be proved to be unitary in $L^2(\mathbb{R})$.
For historical reasons this unitary property is sometimes called the Plancherel theorem and can be seen as a more general then the isometric property defined as Plancherel theorem in this text. 
The unitary property essentially being the original deviation used at the start of this section while isometric property begin the $f=g$ special case.

\subsection{Riemann-Lebesgue Lemma}
The Riemann-Lebesgue lemma states that for all functions in $L^1(\mathbb{R})$ we have:
\[\lim_{|k|\rightarrow \infty}|\hat{f}(k)|= 0  \]

Start by proving the lemma for the subset of $L^1(\mathbb{R})$ call the step function. 
Step functions are defined as linear combinations of indicator functions on open intervals:
\[g(x) = \sum_{n}c_n\chi_{(\mu_n-\delta_n,\mu_n+\delta_n)}(x)\]
By definition have:
\begin{equation*}
\begin{aligned}
\hat{g}(k) =& \sum_nc_n\hat{\chi}_{(\mu_n-\delta_n,\mu_n+\delta_n)}(k)\\
=& \sum_nc_n2k^{-1}\sin\left(k\delta_n\right)\exp(-i\mu_n k) \\
=& 2k^{-1}\sum_nc_n\sin\left(k\delta_n\right)\exp(-i\mu_n k)
\end{aligned}
\end{equation*}

By the triangle inequality we have:
\begin{equation*}
\begin{aligned}
\lim_{|k|\rightarrow \infty}|\hat{g}(k)| =& 2\lim_{|k|\rightarrow \infty}|k^{-1}|\lim_{|k|\rightarrow \infty}\left|\sum_nc_n\sin\left(k\delta_n\right)\exp(-i\mu_n k)\right| \\
\leq& 2\lim_{|k|\rightarrow \infty}|k^{-1}|\sum_n\lim_{|k|\rightarrow \infty}\left|c_n\sin\left(k\delta_n\right)\exp(-i\mu_n k)\right| \\
=& 2\lim_{|k|\rightarrow \infty}|k^{-1}|\sum_n|c_n|\lim_{|k|\rightarrow \infty}\left|\sin\left(k\delta_n\right)\right| \\
\leq&2\lim_{|k|\rightarrow \infty}|k^{-1}|\sum_n|c_n| \\
=& 0\\
\end{aligned}
\end{equation*}

To prove the lemma for the general case we approximate an arbitrary function in $L^1(\mathbb{R})$ with a step function.
This works because step functions are dense in $L^1(\mathbb{R})$, meaning for all $f$ and $\epsilon > 0$ there exists a step function $g$ such that:
\[\|f-g\| < \epsilon\]
This inequality rewritten in terms of the Fourier transforms:
\begin{equation*}
\begin{aligned}
|\hat{f}(k)-\hat{g}(k)| =&\left|\int_{-\infty}^{\infty}(f(x)-g(x))\exp(-ikx)\,dx\right| \\
\leq& \int_{-\infty}^{\infty}|(f(x)-g(x))\exp(-ikx)|\,dx  \\
=&\int_{-\infty}^{\infty}|f(x)-g(x)|\,dx  \\
=&\|f-g\|\\
<& \epsilon \\
\end{aligned}
\end{equation*}

Combining both these inequalities with the triangle inequality gives:
\[\lim_{|k|\rightarrow \infty}|\hat{f}(k)| \leq \lim_{|k|\rightarrow \infty}|\hat{f}(k)-\hat{g}(k)| + \lim_{|k|\rightarrow \infty}|\hat{g}(k)| < \epsilon\]

Since $\epsilon$ can be arbitrary small we have:
\[\lim_{|k|\rightarrow \infty}|\hat{f}(k)| = 0\]

I like this proof because it shows how well behaved $L^1(\mathbb{R})$ is and in particular how the step functions being dense means integration works how you would expect it to work.

It can be used in formal proofs of the Stationary phase approximation and Method of steepest descent, which seem interesting.

\section{Probability}
\subsection{Characteristic Function}

Assume that the function $f(x)$ satisfies the conditions to be a probability density function.
Statisticians define the characteristic function of this distribution as:
\[\phi_X(k) = \mathbb{E}[\exp(ikX)]\]
Applying the law of the unconscious statistician we find that:
\begin{equation*}
\begin{aligned}
\mathbb{E}[\exp(ikX)] =& \int_{-\infty}^{\infty}f(x)\exp(ikx)\,dx\\
=& \hat{f}(-k)\\
\end{aligned}
\end{equation*}

The characteristic function is just the Fourier transform in disguise!
But to we why this expression might be useful consider the following expansion:
\begin{equation*}
\begin{aligned}
\hat{f}(k) =& \mathbb{E}[\exp(-ikX)]\\
=& \mathbb{E}\left[\sum_{n=0}^\infty\frac{(-ik)^n}{n!}X^n\right]\\
=& \sum_{n=0}^\infty\frac{(-ik)^n}{n!}\mathbb{E}\left[X^n\right]\\
=&1-ik\mu-\frac{k^2}{2}\big(\sigma^2+\mu^2\big) + O(k^3)\\
\end{aligned}
\end{equation*}

The characteristic function is like a generating function for statistical moments.
While this expansion is useful for pedagogical reasons we can find a form better for analytical work:
\begin{equation*}
\begin{aligned}
\hat{f}(k) =& \mathbb{E}[\exp(-ikX)]\\
=& \mathbb{E}\left[\exp\left(-ik(X-\mu)-ik\mu\right)\right]\\
=& \exp\left(-ik\mu\right)\mathbb{E}\left[\exp\left(-ik(X-\mu)\right)\right]\\
=& \exp\left(-ik\mu\right)\left(1-\frac{1}{2}k^2\sigma^2+O(k^3)\right)\\
=& \exp\left(-ik\mu-\frac{1}{2}k^2\sigma^2\right)\left(1+O(k^3)\right)\\
\end{aligned}
\end{equation*}

Although the choice was feels arbitrary the elegance of the resulting form and the fact that the choice effects higher orders of k feels it should be optimal.
Following is a promising identity to trying to prove optimality. 
\begin{equation*}
\begin{aligned}
\hat{f}(k)=& \exp\left(-ik\mu-\frac{1}{2}k^2\sigma^2\right)\mathbb{E}\left[\exp\left(-ik(X-\mu)+\frac{1}{2}k^2\sigma^2\right)\right]\\
=& \exp\left(-ik\mu-\frac{1}{2}k^2\sigma^2\right)\mathbb{E}\left[\exp\left(-\frac{1}{2\sigma^2}\left((X-\mu+ik\sigma^2)^2-(X-\mu)^2\right)\right)\right]\\
\end{aligned}
\end{equation*}
The first bit is the mean and standard deviation and the second bit depends only on the standardized centralised moments.
It feels like the first bit deals with location and scale so the second bit can focus on more important issues.

\subsection{Gaussian Distribution}
This also lets us intuit the transform of a Gaussian, if we know that the transform of a Gaussian is another Gaussian.
Let:
\[f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right)\]
We observe that:
\begin{equation*}
\begin{aligned}
\exp(-ik\mu)\exp\left(-k^2\frac{\sigma^2}{2}\right) =& \left(1-ik\mu -k^2\frac{\mu^2}{2} + O(k^3)\right)\left(1-k^2\frac{\sigma^2}{2}+O(k^3)\right)\\
	=&1-ik\mu-\frac{k^2}{2}\big(\sigma^2+\mu^2\big) + O(k^3)\\
\end{aligned}
\end{equation*}

\subsection{Uniform Distribution}
The variance of a uniform distribution on $(a,b)$ is given by:
\[\sigma^2 = \frac{(a-b)^2}{12}\]
It can be a bit difficult to remember the exact denominator, but what if we remembered the Fourier transform of the indicator function?

The uniform distribution can be obtained from scaling the indicator function:
\[f(x) = \frac{1}{2\delta}\chi_{(\mu-\delta,\mu+\delta)}(x)\]
Which has a transform of:
\begin{equation*}
\begin{aligned}
\hat{f}(k) =& \exp(-ik\mu)\frac{\sin(k\delta)}{k\delta}\\ 
=& \exp(-ik\mu)\left(1-\frac{k^2}{6}\delta^2+ O(k^4)\right)\\ 
\end{aligned}
\end{equation*}
Hence we obtain:
\[\sigma^2 =\frac{\delta^2}{3} = \frac{(a-b)^2}{12}\]
As expected.

\subsection{Central Limit Theorem}
Let there be an enumerated family of functions:
\[\hat{f_n}(k) = \exp\left(-ik\mu_n-\frac{1}{2}k^2\sigma^2_n\right)(1+R_n(k))\]
Define:
\[M_N = \frac{1}{N}\sum_{k=1}^{N}\mu_N,\quad S_N^2 = \frac{1}{N^2}\sum_{k=1}^{N}\sigma^2_k\]

\begin{equation*}
\begin{aligned}
\prod_{n=1}^N\hat{f}_n\left(\frac{k}{N}\right) =&\exp\left(-ikM_N-\frac{1}{2}k^2S^2_N\right)\prod_{n=1}^{N}\left(1+R_n\left(\frac{k}{N}\right)\right) \\
\end{aligned}
\end{equation*}

%This can be useful to prove the central limit theorem through the convolution theorem.
%For some general case where we have the same $\mu_n=0$ but varying variances:
%\begin{equation*}
%\begin{aligned}
%	\prod_{n=1}^{N}\left(1-\frac{k^2}{2}\sigma_n^2\right) \\
%\end{aligned}
%\end{equation*}
%To make sense of this let:
%\[\sigma_n^2 = \sigma^2 + \epsilon_n,\,k=\frac{t}{\sigma\sqrt{N}}\]
%Additionally let $e_n$ be the \hyperref[appendix:ESP]{elementary symmetric polynomial} of the $\epsilon_n$
%\begin{equation*}
%\begin{aligned}
%	&\prod_{n=1}^{N}\left(1-\frac{k^2}{2}\sigma_n^2\right) \\
%	=&\prod_{n=1}^{N}\left(1-\frac{t^2}{2}\frac{\sigma^2+\epsilon_n}{N\sigma^2}\right) \\
%	=&\prod_{n=1}^{N}\left(1-\frac{t^2}{2N}-\frac{t^2\epsilon_n}{2N\sigma^2}\right) \\
%	=&\sum_{n=0}^{N}\left(1-\frac{t^2}{2N}\right)^{N-n}\left(\frac{t^2}{2N\sigma^2}\right)^ne_n \\
%\end{aligned}
%\end{equation*}
%Assume that for $N > 0$ we have the limit $\frac{e_n}{N^n} \rightarrow 0$ for some choice of $\sigma$.
%\begin{equation*}
%\begin{aligned}
%	&\sum_{n=0}^{N}\left(1-\frac{t^2}{2N}\right)^{N-n}\left(\frac{t^2}{2N\sigma^2}\right)^ne_n \\
%	&\exp\left(1-\frac{t^2}{2}\right)+\sum_{n=1}^{N}\exp\left(1-\frac{t^2}{2}\right)\left(\frac{t^2}{2\sigma^2}\right)^n\cdot0 \\
%\end{aligned}
%\end{equation*}

\subsection{Cauchy Distribution}
Consider the following function:
\[f(x;x_0,\gamma) = \frac{1}{\pi}\left[\frac{\gamma^2}{(x-x_0)^2+\gamma^2}\right]\]
This is known to most as the Cauchy distribution, to physicists as the Lorentz distribution, and to 17$^{\text{th}}$ century mathematicians as the ``Witch of Agnesi".
Other then being a cool name witch may be appropriate because it's a common pathological function as despite meeting all the requirements to be a distribution it has no mean or variance. 

Dispute these problems we do have clear position, $x_0$, and scale, $\gamma$, parameters that we would like to understand better.
This distribution has a well defined Fourier transform, from the transform lookup table, we have:
\[\hat{f}(k;x_0,\gamma) = \exp(-ix_0k-\gamma|k|)\]
And looks remarkable similar to the transform of a Gaussian.
Despite not being the mean $x_0$ does act exactly as the mean would.
And $|k|$ is not unlike $k^2$, which makes $\gamma$ like the variance.

This demonstration how the characteristic function can be useful as it can extend our understanding away from functions with a defined mean and variance to a functions which have a characteristic functions.
The Cauchy distribution lacking a defined mean and variance isn't just an academic concern as the distribution arises from:
\begin{enumerate}
\item[Spectroscopy] The shape of many types pf spectral lines is the Cauchy distribution.
\item[Hydrology] Extreme events like maximum one-day rainfalls and river discharges are modeled with the Cauchy distribution.
\item[Finance] When modeling extrema risk the Cauchy heaver tails makes a better model then a Gaussian.
\item[Statistics] The ration of two independent, normally distributed, zero mean, variables is the Cauchy distributed.
\item[Electronics] The imaginary component of electrical permittivity is the Cauchy distribution.
\end{enumerate}

Note how a lot of these application have to do with modeling extrema values, this is because the Cauchy distribution has heavy tails, this is also why the moments don't exist.

\subsection{Mean}
Using the innerproduct from the Plancherel section we have:
\[\int_{-\infty}^{\infty}x|f(x)|^2\,dx = \frac{i}{2\pi}\int_{-\infty}^{\infty}\overline{\hat{f}(x)}\hat{f}'(x)\,dx\]

Which is already promising since the integral is primed for change of varitable.
But lets consder the case when $f$ is purely real.
\[\int_{-\infty}^{\infty}xf(x)^2\,dx = -\frac{1}{2\pi}\int_{-\infty}^{\infty}\Im[\overline{\hat{f}(x)}\hat{f}'(x)]\,dx\]

Since $f$ is real we can easily get the \hyperref[appx:real-img-odd-even]{real and imaginary components} of $\hat{f}$ and $\hat{f}'$:
\begin{equation*}
\begin{aligned}
	\hat{f}(x) = & \frac{1}{2}(\hat{f}(x)+\hat{f}(-x))+i\frac{1}{2}(\hat{f}(x)-\hat{f}(-x)) \\
	\hat{f}'(x) = & \frac{1}{2}(\hat{f}'(x)-\hat{f}'(x))+i\frac{1}{2}(\hat{f}'(x)+\hat{f}'(-x)) \\
\end{aligned}
\end{equation*}

Hence:
\begin{equation*}
\begin{aligned}
	-\frac{1}{2\pi}\int_{-\infty}^{\infty}\Im[\overline{\hat{f}(x)}\hat{f}'(x)]\,dx =& 
	-\frac{1}{2\pi}\int_{-\infty}^{\infty}\left(\frac{1}{2}(\hat{f}(x)+\hat{f}(-x))-i\frac{1}{2}(\hat{f}(x)-\hat{f}(-x))\right)\left(\frac{1}{2}(\hat{f}'(x)-\hat{f}'(-x))+i\frac{1}{2}(\hat{f}'(x)+\hat{f}'(-x))\right)\,dx \\
\end{aligned}
\end{equation*}


\section{Hermite Polynomials}
% From them being eigenfunctions there is an easy relationship between the Hermite expansion of a function and its transform.
% Can be used to calculate operations on the coefficients Hermite polynomials.
% Nice discrete system?
% Use probabilists' for the eigenfunction properties
\subsection{Definitions}
Due to their wide use there are many definitions of the Hermite polynomials.
In my view there are two definitions which capture most of their properties.

The first is through a recursive relation which allow for efficient calculation and show their polynomial nature:
\[H_0(x) = 1,\, H_{n+1}(x) = 2xH_n(x)-H'_n(x)\]

The second definition is their exponential generating function which shows their relationship to the Gaussian function:
\[\exp(2xt-t^2) = \sum_{n=0}^{\infty}H_n(x)\frac{t^n}{n!}\]

To get a sketch of why these definitions are equivalent observe the following:
\[\frac{\partial}{\partial t}\exp(2xt-t^2) = 2x\exp(2xt-t^2)-\frac{\partial}{\partial x}\exp(2xt-t^2) \]

Recalling that taking the derivative of an exponential generating function down shifts the constants we obtain:
\begin{equation*}
\begin{aligned}
\sum_{n=0}^\infty H_{n+1}(x)\frac{t^n}{n!} =& 2x\sum_{n=0}^{\infty}H_n(x)\frac{t^n}{n!}-\frac{\partial}{\partial x}\sum_{n=0}^\infty H_n(x)\frac{t^n}{n!}\\
=& \sum_{n=0}^\infty\bigg(2xH_n(x)-H'_n(x)\bigg)\frac{t^n}{n!}
\end{aligned}
\end{equation*}

Along with the two main definitions there's also an interesting Rodrigues' formula:
\[H_n(x) = (-1)^n\exp(x^2)\frac{d^n}{d x^n}\exp(-x^2)\]

And a rescaling used in probability called the probabilists' Hermite polynomials:
\[He_n(x) = 2^{-\frac{n}{2}}H_n\left(\frac{x}{\sqrt{2}}\right)\]
\subsection{Orthogonality} 
To see the Hermite polynomial orthogonality we wish to evaluate integrals of the following form:
\[\int_{-\infty}^{\infty}H_n(x)H_m(x)\exp(-x^2)\,dx\]

This can be easily done by using a double exponential generating function and the Hermite polynomials exponential generating function:
\begin{equation*}
\begin{aligned}
&\sum_{n=0}^{\infty}\sum_{m=0}^{\infty}\int_{-\infty}^{\infty}H_m(x)H_n(x)\exp(-x^2)\,dx\frac{t_0^n}{n!}\frac{t_1^m}{m!}\\
=&\int_{-\infty}^{\infty}\exp(-x^2)\sum_{n=0}^{\infty}H_n(x)\frac{t_0^n}{n!}\sum_{m=0}^{\infty}H_m(x)\frac{t_1^m}{m!}\,dx \\
=&\int_{-\infty}^{\infty}\exp(-x^2)\exp(2xt_0-t_0^2)\exp(2xt_1-t_1^2)\,dx \\
=&\exp(2t_0t_1)\int_{-\infty}^{\infty}\exp(-(x-t_0-t_1)^2)\,dx\\
=&\exp(2t_0t_1)\sqrt{\pi}\\
=&\sum_{n=0}^{\infty}2^n\sqrt{\pi}\frac{t_0^nt_1^n}{n!} \\
=&\sum_{n=0}^{\infty}\sum_{m=0}^{\infty}2^n\sqrt{\pi}n!\delta_{nm}\frac{t_0^nt_1^m}{n!m!} \\
\end{aligned}
\end{equation*}
By equating coefficients of $t_0,\,t_1$ we get the relation:
\[\int_{-\infty}^\infty H_n(x)H_m(x)\exp(-x^2)\,dx = \sqrt{\pi}2^nn!\delta_{nm}\]

\subsection{Fourier Transform}
Hermite polynomials are important for us because of their involvement in the eigenfunction of the Fourier transform.
To see how Hermite polynomials are involved in the Fourier transform consider the following function:
\[f(x) = \exp\left(-\frac{1}{2}x^2+2xt-t^2\right) = \sum_{n=0}^\infty\exp\left(-\frac{x^2}{2}\right)H_n(x)\frac{t^n}{n!}\]
This function can be refactored into a form where the Fourier transform of $x$ can be seen:
\[f(x) = \exp\left(-\frac{1}{2}(x-2t)^2\right)\exp(t^2)\]
Hence the transform of $f$ is simply:
\[\hat{f}(k) = \sqrt{2\pi}\exp(-2ikt)\exp\left(-\frac{k^2}{2}\right)\exp(t^2)\]
Expanding the two non $k^2$ exponentials using the exponential generating function definition on the Hermite polynomials gives:
\[\hat{f}(k) = \sum_{n=0}^{\infty}\sqrt{2\pi}(-i)^n\exp\left(-\frac{k^2}{2}\right)H_n(k)\frac{t^n}{n!} \]
Since the Fourier transform is linear we can equate the $t^n$ terms to get:
\[\exp\left(-\frac{x^2}{2}\right)H_n(x) \rightarrow \sqrt{2\pi}(-i)^nH_n(k)\exp\left(-\frac{k^2}{2}\right)\]

\subsection{Expansions}
Let:
\begin{equation*}
\begin{aligned}
	f(x) =& \exp(-(\sqrt{a+1}x-t)^2)\exp(-at^2) \\
	=& \exp(-(a+1)x^2+2\sqrt{a+1}xt-(a+1)t^2)\\
	=& \exp(-ax^2)\exp(-x^2+2\sqrt{a+1}xt-(a+1)t^2)\\
	=& \sum_{n=0}^{\infty}H_n(x)\exp(-ax^2)\exp(-x^2)\frac{(\sqrt{a+1}t)^n}{n!}
\end{aligned}
\end{equation*}

\[\exp(-ax^2) = \sum_{n=0,2,4...}^\infty H_n(x)\frac{\sqrt{2}}{n!}\left(\frac{-a}{2\sqrt{a^2+1}}\right)^n\]

\subsection{Eigenfunction}
Since the given eigenfunctions are all the same bar a phase we should be able to break a function up into 4 functions where each gets a quarter phase by transform.
Consider again that \[f(x,t) = \exp(2xt-t^2) = \sum_{n=0}^\infty H_n(x)\frac{t^n}{n!}\]

\begin{equation*}
\begin{aligned}
A_N(x,t) =& \frac{1}{4}\big(f(x,t)+(-1)^Nf(x,-t)+i^Nf(x,it)+(-i)^Nf(x,-it)\big)\\
=& \sum_{n=0}^\infty H_n(x)\frac{t^n+(-1)^N(-t)^n+i^N(it)^n+(-i)^N(-it)^n}{n!} \\
\end{aligned}
\end{equation*}

\section{Chebyshev Polynomials}
Like Hermite Polynomials the Chebyshev Polynomials are orthogonal polynomials that have multiple definitions.

The two main ones are trigonometric substitution:
\[T_n(\cos(\theta)) = \cos(n\theta)\]

A recursive definition:
\begin{equation*}
\begin{aligned}
	T_0(x) =& 1 \\
	T_1(x) =& x \\
	T_{n+1}(x) =& 2xT_n(x) - T_{n-1}(x)\\
\end{aligned}
\end{equation*}

One important application is Chebyshev-Gaussian Quadrature where a function $f$ is approximated through through series expansion in Chebyshev polynomials then integrated.
Chebyshev-Gaussian Quadrature is an open topic of research where an open question being it's unreasonable effectiveness.
That is to say this expansion give a more accurate approximation of the integral than one might expect.

The obvious idea explanation is that this form of approximation takes all the low frequency term and that high frequency terms are likely to cancel.
Lets explore this with out new Fourier tools.

\subsection{Basics}
Let $f$ be a function with Fourier transformation $\hat{f}$.
Lets approximate $f$ by:

\begin{equation*}
\begin{aligned}
\bar{f}(x) =& \frac{1}{2\pi}  \int_{-a}^{a}\hat{f}(k)\exp(ikx)\,dk\\
=& \frac{1}{2\pi}  \int_{-\infty}^{\infty}\hat{f}(k)\chi_{(-a,a)}(k)\exp(ikx)\,dk\\
\end{aligned}
\end{equation*}

Recall that $\chi_{(-a,a)}(k)$ is the Fourier Transform of $(x\pi)^{-1}\sin(ax)$.
So by the convolution theorem we have, up to countably infinite points:
\[ \bar{f}(x) = \pi^{-1}\int_{-\infty}^{\infty}f(x-t)t^{-1}\sin(at)\,dt\]

Now let:
\[f(x) = \sum_{n=-\infty}^{\infty}f_n\exp(ixn)\]
(Close enough to a Chebyshev Expansion, just scale and change signs)
We have:
\[\bar{f}(x) = \sum_{-a \leq n \leq a}f_n\exp(ixn)\]


\begin{equation*}
\begin{aligned}
\bar{f}(x) =& \pi^{-1}\int_{-\infty}^{\infty}t^{-1}\sin(at)\sum_{n=-\infty}^{\infty}f_n\exp(in(x-t))\,dt \\
=& \pi^{-1}\sum_{n=-\infty}^{\infty}f_n\exp(inx)\int_{-\infty}^{\infty}t^{-1}\sin(at)\exp(-int)\,dt \\
\end{aligned}
\end{equation*}
\section{Airy Function}
This is a short and sweet application, proving a closed form satisfies a differential equation.
One definition of the Airy function is:
\[\Ai(x) = \frac{1}{\pi}\int_{0}^{\infty}\cos\left(\frac{t^3}{3}+xt\right)\,dt\]
Let us change the definition to see how Fourier transform is present:
\begin{equation*}
\begin{aligned}
	\Ai(x) =& \frac{1}{\pi}\int_{0}^{\infty}\cos\left(\frac{t^3}{3}+xt\right)\,dt\\
	=&\frac{1}{2\pi}\int_{0}^{\infty}\left(\exp\left(i\left(\frac{t^3}{3}+xt\right)\right)+\exp\left(-i\left(\frac{t^3}{3}+xt\right)\right)\right)\,dt\\
	=&\frac{1}{2\pi}\int_{-\infty}^{\infty}\exp\left(i\left(\frac{t^3}{3}+xt\right)\right)\,dt\\
\end{aligned}
\end{equation*}
Observe that this is the definition of the Fourier transform, hence:
\[\hat{\Ai}(k) = \exp\left(\frac{i}{3}k^3\right)\]

For the next step we apply both the derivative law and it's dual, so clarity is key.
First we transform the Airy function \underline{after} deriving:
\begin{equation*}
\begin{aligned}
\widehat{\Ai''}(k) =& (ik)^2\hat\Ai(k) \\
=&-k^2\exp\left(\frac{i}{3}k^3\right)\\
\end{aligned}
\end{equation*}

Next we times the Airy function by $x$ \underline{before} transforming.
By the dual of the derivative rule derive \underline{after} transforming:
\begin{equation*}
\begin{aligned}
\widehat{x\Ai}(k) =& i\frac{d}{d\,k}\hat\Ai(k) \\
=&-k^2\exp\left(\frac{i}{3}k^3\right)\\
\end{aligned}
\end{equation*}

%\[\int_{-\infty}^{\infty}\Ai(x)\exp(-ixt)\,dx = \exp\left(i\frac{t^3}{3}\right)\]
%From the derivative law we have:
%\[\int_{-\infty}^{\infty}\Ai''(x)\exp(-ixt)\,dx = (it)^2\exp\left(i\frac{t^3}{3}\right)\]
%From simply deriving the Fourier definition gives:
%\[\int_{-\infty}^{\infty}\Ai(x)(-ix)\exp(-ixt)\,dx = it^2\exp\left(i\frac{t^3}{3}\right)\]

Hence both $\Ai''(x)$ and $x\Ai(x)$ share $-t^2\exp\left(i\frac{t^3}{3}\right)$ as a Fourier transform.
Making them the same meaning $\Ai$ satisfy.
\[\Ai''(x) = x\Ai(x)\]
The simplest non linear second order differential equation imaginable, from which lots of applications follow.
\\

\begin{equation*}
\begin{aligned}
\int_{-\infty}^{\infty}f(t+x)f(t)\,dt =&\int_{-\infty}^{\infty}f(t+x)\overline{f(t)}\,dt \\
=&\frac{1}{2\pi}\int_{-\infty}^{\infty}\exp(ikx)\hat{f}(k)\overline{\hat{f}(k)}\,dk \\
=&M\delta(x) \\
\end{aligned}
\end{equation*}

Prove orthogonality by Plancherel 
And the fact that magnitude of the Fourier transform never changes
\\

We can use this method to solve the general:
\[y'' = xy+ay\]
\[(ik)^2f = if'+af\]
\section{Uncertainty Principle}
I don't remember how to prove this exactly but I have all the pieces and want to come back to it without looking it up.
\[\int_{-\infty}^{\infty}x^2|f(x)|^2\,dx\int_{-\infty}^{\infty}x^2|\hat{f}(x)|^2\,dx = \langle xf(x),xf(x) \rangle \langle x\hat{f}(x), x\hat{f}(x) \rangle\]
Cauchy-Schwarz inequality:
\[\langle g,g \rangle\langle h,h\rangle \geq |\langle g,h \rangle|^2\]
Plancherel:
\[\langle g,h \rangle = \frac{1}{2\pi}\langle \hat{g},\hat{h}\rangle\]
In particular:
\[\langle xg(x),xh(x) \rangle = \frac{1}{2\pi}\langle \hat{g}'(x),\hat{h}'(x)\rangle\]
Taking only the imaginary component:
\[|\langle g,h \rangle|^2 \geq \frac{1}{4}(\langle h,g \rangle - \langle g,h \rangle)^2\]

These all go together somehow.

Also note that:
\[\int_{-\infty}^{\infty}x^2|f(x)|^2\,dx = \int_{-\infty}^{\infty}|\hat{f}'(x)|^2\,dx\]
These are both measures of dispersion.
To see this look at the right hand side with a standard Gaussian:
\[f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2}\left(\frac{x}{\sigma}\right)^2 \right)\]
We get:
\[\int_{-\infty}^{\infty}|f'(x)|^2\,dx = \frac{1}{4\sqrt{\pi}\sigma^3} \]

\section{Nyquist–Shannon Sampling Theorem}
Consider the convolution of a function with a Dirac comb (of step size $B$):
\[g(x) = \int_{-\infty}^{\infty}f(x-t)\sum_{n=-\infty}^{\infty}\delta(t-nB)\,dt = \sum_{n=-\infty}^{\infty}f(x-nB)\]
Then $g$ is the sum of infinite copies of $f$ spaced $B$ apart, if such a sum exists.
Hence $g$ is periodic with a period of $B$. 
Proof of this can be seen by seeing that translation by $NB$ shifts the index $n$ of the sum across by $N$ but keeps all the term the same:
\[g(x+NB) = \sum_{n=-\infty}^{\infty}f(x+NB-nB) = \sum_{n=-\infty}^{\infty}f(x-(n-N)B) \]
\\

In particular if $f$ is zero outside of $(-B,B)$ then the copies can't overlap, making them perfect copies.
Meaning if we multiply the convolution by the indicator function of $(-B,B)$ we recover $f$!
\[f(x) = \chi_{(-B,B)}(x)\int_{-\infty}^{\infty}f(x-t)\Sha_B(t)\,dt\]
\\

Since we do all this work only to get $f$ back it may seem like a waste of time.
But consider a function $f$ such that $\hat{f}$ is zero outside of $(-B,B)$.
Then we have:
\[\hat{f}(k) = \chi_{(-B,B)}(k)\int_{-\infty}^{\infty}\hat{f}(k-t)\Sha_B(t)\,dt\]
The RHS is a combination of operations and functions we know the inverse Fourier transform of meaning we get a new expression for $f$:
\[f(x) = \int_{-\infty}^{\infty}\left(2\frac{\sin(D(x-t))}{x-t}\right)\left(f(t)\frac{2\pi}{|B|}\Sha_{\frac{2\pi}{B}}(t)\right)\,dt \]
Letting $T = \frac{2\pi}{|B|}$ we get:
\[ f(x) = 2T\sum_{n=-\infty}^{\infty}f(Tn)\frac{\sin(D(x-nT))}{x-nT} \]
(TODO: check that all the coefficients are correct).
\\

We have reconstructed a continuous function $f$ as an discrete sum of it's samples $f$.
This is called Nyquist–Shannon Sampling Theorem and along with being cool is a very useful for any application that tries to reconstruct a signal from samples.

\subsection{Strictness of $B$}
Note that $\hat{f}$ can only be non-zero on $(-B,B)$ this strictly includes $\hat{f}(\pm B) = 0$ for the reconstruction of $f$ to be unique.

Consider the class of functions $f(x) = a\sin\left(\frac{2\pi}{T}x\right)$.
Their transform's are all deltas at $\pm B$ but when sampled all $f(nT) = 0$ hence they can't be distinguished.
\\

This is a particular pathological examples because $\hat{f}$ aren't just non-zero, but infinite. 
But it demonstrates the principles of the strictness of $B$.

\chapter{Generalizations}
\section{Pontryagin Duality}
Let $G$ be a locally compact group and let $\hat{G}$ be the group of continuous group homomorphism from $G$ to the circle group $\Hom(G,T)$. Where $\hat{G}$ is endowed with the compact-open topology.

Then there is a canonical isomorphism between $G$ and $\hat{\hat{G}}$.

Note that locally compact groups come with an interesting measure called the "Haar measure" which interacts well with this transform.

\section{Discrete Fourier transform over a ring}
\label{sec:ring}
Observe that many of the properties of used in the definition of the Discrete Fourier require only that the underlying set be a ring with a special element $\alpha$ such that $\alpha^n = 1$ and $\sum_{k=0}^{n-1}\alpha^{kj} = n\delta_{j,n}$.
This lets us expand discussion of the Discrete Fourier transform to any ring with this element.
And also explains why some prefer the non-unitary definition as a square root is a significant jump from a ring.

\subsection{Definition}
Let $a_k$ be a n-tuple of ring elements, $\alpha$ be an element of that ring with the above properties. 
For convenience let $n$ have a multiplicative inverse, and define a new tuple as:
\[A_k = \sum_{j=0}^{n-1}a_j\alpha^{jk}\]

Then we have:
\begin{equation*}
\begin{aligned}
	\frac{1}{n}\sum_{i=0}^{n-1}A_i\alpha^{-ik} =& 
	\frac{1}{n}\sum_{i=0}^{n-1}\left(\sum_{j=0}^{n-1}a_j\alpha^{ji}\right)\alpha^{-ik} \\ 
	=&\frac{1}{n}\sum_{j=0}^{n-1}a_j\sum_{i=0}^{n-1}\alpha^{i(j-k)} \\ 
	=&\frac{1}{n}\sum_{j=0}^{n-1}a_jn\delta_{j,k} \\ 
	=&a_k \\ 
\end{aligned}
\end{equation*}

\subsection{Basic Properties}
Other then the obvious linearity there are two other easy and useful properties of the ring transform.
Uniqueness is a corollary of linearity:
\[A_n=B_n \Rightarrow a_n-b_n = \frac{1}{n}\sum_k\alpha^{-kn}(A_k-B_k) = 0\]

A version of Plancherel's Theorem is also easy:
\begin{equation*}
\begin{aligned}
	\sum_k a_kb_k =& \sum_k \left(\frac{1}{n}\sum_iA_i\alpha^{-ki}\right) \left(\frac{1}{n}\sum_jB_j\alpha^{-kj}\right) \\
	=&  \frac{1}{n^2}\sum_i\sum_jA_iB_j \sum_k\alpha^{-k(i+j)}  \\
	=&  \frac{1}{n^2}\sum_i\sum_jA_iB_j \sum_k\alpha^{k((n-i)-j)}  \\
	=&  \frac{1}{n}\sum_iA_iB_{n-i}  \\
\end{aligned}
\end{equation*}

I'm sure similar results for convolution also follow.

\subsection{Integral Domains}
It's worth noting that in integral domains the requirements for $\alpha$ are simplified as:
\[(\alpha^k-1)\sum_{i=0}^{n-1}\alpha^{ik} = \alpha^{nk}-1 = 0\]
Means it is sufficient for $\alpha$ to be a primitive nth root of unity.
It's further worth noting that all fields are integral domains.

\appendix	
\chapter{Appendix}

\section{Convention}
In this text the definition of Fourier and inverse Fourier transform are given by:
\[\hat{f}(k) = \int_{-\infty}^{\infty}f(x)\exp(-ikx)\,dx\]
\[f(x) =\frac{1}{2\pi} \int_{-\infty}^{\infty}\hat{f}(x)\exp(ikx)\,dk\]

This is called the "non-unitary, angular frequency" convention and it's clear there is some arbitrariness around the certain coefficients as can see by defining a new similar transform:
\[F(k) = \frac{1}{a}\int_{-\infty}^{\infty}f(x)\exp(-ibxk)\,dx\]

We can go between different choices of $a$ and $b$ by mere scaling, which the basic properties section shows isn't a big deal:
\[\frac{a}{A}F\left(\frac{B}{b}k\right) = \frac{1}{A}\int_{-\infty}^{\infty}f(x)\exp(-iBxk)\,dx\]

Since the choice is ours to make it's natural to ask what the most convent choice is.
An immediate concern is if the problem attempting to be solved using the Fourier transform has a natural choice.
In some physical applications choosing $b= 1,\pi,2\pi$ is natural.

Following any natural application the next concern is to make sure the inverse transform works.
Using the same argument as in the \hyperref[sec:inv-trans]{inverse function section} we get the following relation:
\[\frac{1}{c}\int_{-\infty}^{\infty}F(k)\exp(-idxk)\,dk = \frac{2\pi}{ac}f\left(\frac{d}{b}x\right)\]

So if we choose $d=b$ and $ac = 2\pi$ the inverse works and we are still free to choose $a$ and $b$ as we please.
\\

From the \hyperref[sec:plancherel]{Plancherel's Theorem} we see that the transform being unitary would be convent and that this can be achieved with:
\[a = c = \sqrt{2\pi}\]
But this is presents a problem, since the use of square roots and $\pi$ make this choice in a large jump up from only using \hyperref[sec:ring]{ring operations}.

This creates a clear branching point between what would be convenient algebraically versus analytically.
With algebra preferring us to stay as simple as possible with $a=1$ and analysis preferring $a=\sqrt{2\pi}$.
\\

I have chosen $a=b=1$ because of how it simplify the definition of the transform but you are free to chose whatever convention you like, provided you keep conversion in mind while looking at tables of transforms.
\\

As ending remarks it is worth noting that in probability theory the characteristic function is equal to the Fourier transform with $a=1,\,b=-1$. 
And that the Pontryagin Duality makes all the arbitrariness disappear by focusing on the more abstract isomorphism between locally compact abelian groups (But even Pontryagin Duality isn't the undisputed "best" viewpoint since it introduces topology but ignores ring operations, which may be more important in certain applications).

\section{Table of Transforms}

\begin{center}
\begin{tabular}{c|c}
	Time domain & Frequency domain \\ \hline
	$\delta(x)$  & $1$ \\
	$1$ & $2\pi\delta(k)$ \\
	$\chi_{(\mu-\delta,\mu+\delta)}(x)$ & $2k^{-1}\sin\left(k\delta\right)\exp(-i\mu k)$ \\
	$f(x) = \begin{cases} 1-2|x|& |x| <\frac{1}{2}\\ 0& \text{else}\end{cases}$ & $2\sinc\left(\frac{k}{4}\right)^2$ \\
	$\sum_{n=-N}^N\delta(x-na)$ & $\frac{\sin\left(\left(N+\frac{1}{2}\right)ka\right)}{\sin\left(\frac{ka}{2}\right)}$\\
	$\sum_{n=-\infty}^\infty\delta(x-na) $ & $\frac{2\pi}{|a|}\sum_{n=-\infty}^\infty\delta\left(k-\frac{2\pi n}{a}\right)$  \\
	$\exp(ixk_0)$ & $ 2\pi\delta(k-k_0)$ \\
	$\sin(k_0x)$ & $\pi i (\delta(k+k_0)-\delta(k-k_0))$\\
	$\cos(k_0x)$ & $\pi  (\delta(k+k_0)+\delta(k-k_0))$\\
	$\exp(-ax^2)$&$ \sqrt{\frac{\pi}{a}}\exp\left(-\frac{-k^2}{4a}\right)$ \\
\end{tabular}
\end{center}

The $\sinc$ function is normalized.

\section{Important integrals}

\subsection{Gaussian Functions}
A reminder that for $a,b \in \mathbb{R}$ such that $a > 0$ we have:
\[\int_{-\infty}^{\infty}\exp(-a(x-b)^2)\,dx = \sqrt{\frac{\pi}{a}}\]
We will need to expand the domain of $b$ to $\mathbb{{C}}$.
To this end observe that $\exp(-z^2)$ is finite for all values of $z$ as:
\begin{equation*}
\begin{aligned}
	|\exp(-(x+iy)^2)| =& |\exp(-x^2+y^2)\exp(-2ixy)|\\
	=& |\exp(-x^2+y^2)| \\
\end{aligned}
\end{equation*}
Now consider $\mathcal{C}$ be the contour $(-t,0)-(t,0)-(t,\Im[b])-(-t,\Im[b])-(-t,0)$.
By the finitude of $\exp(-z^2)$ we have:
\[0 = \int_\mathcal{C}\exp(-z^2)\,dz \]
By the ML bound the magnitude of the left/right sides of the contour are bound by:
\begin{equation*}
\begin{aligned}
	&\left|\int_{0}^{|\Im[b]|}\exp(-(t\pm i\tau)^2)\,d\tau\right|\\
	\leq&|\Im[b]|\max\left\{|\exp(-(t\pm i\tau)^2)|\,\bigg|\,\tau \in (0,|\Im[b]|)\right\}\\
	=&|\Im[b]|\max\left\{|\exp(-t^2 +\tau^2)|\,\bigg|\,\tau \in (0,|\Im[b]|)\right\}\\
	=&|\Im[b]|\exp(-t^2 +|\Im[b]|^2)\\
\end{aligned}
\end{equation*}
And vanish in the limit $t\rightarrow \infty$.
Meaning in the limit we have:
\begin{equation*}
\begin{aligned}
	0 =& \int_\mathcal{C}\exp(-z^2)\,dz \\
	=& \int_{-t}^{t}\exp(-z^2)\,dz + \int_{t}^{-t}\exp(-(z-i\Im[b])^2)\,dz \\
	&\int_{-\infty}^{\infty}\exp(-z^2)\,dz = \int_{-\infty}^{\infty}\exp(-(z-i\Im[b])^2)\,dz \\
\end{aligned}
\end{equation*}
Meaning the integral is unaffected by imaginary translations.
Since we already have that the integral is unaffected by real translations combining the two we have:
\[\int_{-\infty}^{\infty}\exp(-a(x-b)^2)\,dx = \sqrt{\frac{\pi}{a}}\]
For $a \in \mathbb{R}$, $b \in \mathbb{C}$ and $a > 0$.
\\

And in particular:
\[ \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\exp\left(-\frac{1}{2}\sigma^2k^2+ik(x-\mu)\right) \,dx \] 
\subsection{Signed Exponential}
\begin{equation*}
\begin{aligned}
&\int_{-\infty}^{\infty}\exp(-|x|)\exp(-ikx)\,dx \\
=& \int_{0}^{\infty}\exp(-x)\exp(-ikx)\,dx+\int_{-\infty}^{0}\exp(x)\exp(-ikx)\,dx\\
=& \int_{0}^{\infty}\exp(-x)\exp(-ikx)\,dx+\int^{\infty}_{0}\exp(-x)\exp(ikx)\,dx\\
=& 2\int_{0}^{\infty}\exp(-x)\cos(kx)\,dx\\
=& \frac{2}{1+k^2}\\
\end{aligned}
\end{equation*}
\subsection{Cauchy Exploration}
The convolution of a Cauchy with a Gaussian is easily solvable by double deriving by $x$.
\begin{equation*}
\begin{aligned}
I(x) =&\frac{\gamma}{\pi\sqrt{2\pi\sigma}}\int_{-\infty}^{\infty}\frac{1}{\gamma^2+t^2}\exp\left(-\frac{1}{2\sigma^2}(x-t)^2\right)\,dt\\
\end{aligned}
\end{equation*}

\section{Dirac Comb}
\label{appx:dirac-comb}
The Dirac Comb of period $P$ is defined as
\[\Sha_P(x) = \sum_{n=-\infty}^{\infty}\delta(x-nP) \] 

An important property of $\Sha_P$ is that:
\[\Sha_P(x) = \frac{1}{P}\sum_{n=-\infty}^{\infty}\exp\left(2\pi i n \frac{x}{P}\right)\]
This can easily be proven as a Fourier transform but it can also be proven in a more direct and interesting way, consider:
\[f(x) = \sum_{n=-\infty}^{\infty}\exp(2\pi n x)\]

When $\exp(2\pi x) \neq 1$
\begin{equation*}
\begin{aligned}
f(x) =& \sum_{n=-\infty}^{\infty}\exp(2\pi n x) \\
=& \left(\sum_{n=-\infty}^{-1}+\sum_{n=0}^{0}+\sum_{n=1}^{\infty}\right)\exp(2\pi n x) \\
=& \frac{\exp(-2\pi n x)}{1-\exp(-2\pi n x)} + 1 + \frac{\exp(2\pi n x)}{1-\exp(2\pi n x)} \\
=& \frac{-1}{1-\exp(2\pi n x)} + 1 + \frac{\exp(2\pi n x)}{1-\exp(2\pi n x)} \\
=& 1-1\\
=& 0\\
\end{aligned}
\end{equation*}

As for normalization:
\begin{equation*}
\begin{aligned}
\int_{x}^{x+1}f(t)\,dt =& \int_{x}^{x+1}\sum_{n=-\infty}^{\infty}\exp(2\pi n t)\,dt\\
=& 1+\int_{x}^{x+1}\sum_{\stackrel{n=-\infty}{n\neq 0}}^{\infty}\exp(2\pi n t)\,dt\\
=& 1+\sum_{\stackrel{n=-\infty}{n\neq 0}}^{\infty}\int_{x}^{x+1}\exp(2\pi n t)\,dt\\
=& 1+\sum_{\stackrel{n=-\infty}{n\neq 0}}^{\infty}\left[\frac{1}{2\pi n}\exp(2\pi n t)\right]_{x}^{x+1}\\
=& 1+\sum_{\stackrel{n=-\infty}{n\neq 0}}^{\infty}\frac{\exp(2\pi x n)}{2\pi n}(\exp(2\pi n)-1)\\
=& 1\\
\end{aligned}
\end{equation*}

\section{The Parity Operator and Odd, Even, Real, and Imaginary Components}
\label{appx:real-img-odd-even}
The Parity operator takes a function multiplies it's argument by $-1$:
\[\mathcal{P}[f](x) = f(-x)\]
That functions that are symmetric in regards to this operator are called even and ones that are anti-symmetric are called odd.

A function can be uniquely split into  sum of even and odd functions through the following identity:
\[ f(x) = \overbrace{\frac{f(x)+f(-x)}{2}}^\text{Even}+\overbrace{\frac{f(x)-f(-x)}{2}}^\text{Odd}\]

A purely real number is symmetric in regards to the conjugation operator and a purely imaginary number is anti-symmetric.
A function whose output are purely real numbers is called purely real and similarly for imaginary.

A function can be uniquely split into a sum of purely real and imaginary functions through the following identity:
\[ f(x) = \overbrace{\frac{f(x)+\overline{f(x)}}{2}}^\text{Real}+\overbrace{\frac{f(x)-\overline{f(x)}}{2}}^\text{Imaginary}\]

These unique sums combine to uniquely split an arbitrary function into four parts:
\begin{equation*}
\begin{aligned}
	 f(x) =& \overbrace{\frac{f(x)+\overline{f(x)}+f(-x) + \overline{f(-x)}}{4}}^\text{Real and Even}+\overbrace{\frac{f(x)-\overline{f(x)}+f(-x)-\overline{f(-x)}}{4}}^\text{Imaginary and Even} \\
	 &+ \overbrace{\frac{f(x)+\overline{f(x)}-f(-x) - \overline{f(-x)}}{4}}^\text{Real and Odd}+\overbrace{\frac{f(x)-\overline{f(x)}-f(-x)+\overline{f(-x)}}{4}}^\text{Imaginary and Odd}\\
\end{aligned}
\end{equation*}

The formula for adding a third operator and making an eight way sum of terms can be deduced by observation.

\end{document}
