% Copyright 2023 Kieran W Harvie. All rights reserved.

\section{Information Theory based Entropy}
I want to do some entropy/temperature calculations only using motivations from a information perspective.
We will start with the classic problem:
\begin{equation*}
\begin{aligned}
	\text{Maximize: }& H=-\sum_i p_i\ln(p_i)\\
	\text{Subject to: }& \sum_ip_i=1,\quad \sum_ip_iw_i = \mu \\
\end{aligned}
\end{equation*}
Start with a standard Lagrangian:
\[ L = -\sum_ip_i\ln(p_i)-a\left(\sum_i p_i - 1\right) - b\left(\sum_i p_iw_i - \mu\right)\]
Giving:
\[p_i = \exp(-1-a-bw_i)\]
We can easily remove $a$ by normalizing:
\[p_i = \frac{1}{Z(b)}\exp(-bw_i)\]
Where:
\[Z(t) = \sum_i \exp(-tw_i)\]
Likewise for $\mu$:
\[\mu = \sum_ip_iw_i = \frac{1}{Z(b)}\sum_iw_i\exp(-bw_i)\]
Substituting back into $H$:
\begin{equation*}
\begin{aligned}
	H =& \frac{1}{Z(b)}\sum_i\bigg(\exp(-bw_i)(bw_i+\ln(Z(b)))\bigg)\\
	=& \frac{1}{Z(b)}\bigg(bZ(b)\mu+Z(b)\ln(Z(b))\bigg)\\
	=& b\mu+\ln(Z(b))\\
\end{aligned}
\end{equation*}

The $b\mu$ term already lets us interpret $b^{-1}$ as acting like temperature.
As we'd expect an increase of $\mu$ to allows us to increase entropy because it allows us to distribute more probability to larger $w_i$.
\\

Let go further by noticing that:
\[\mu = - \frac{Z'(b)}{Z(b)} = -\frac{d}{d\,b}\ln(Z(b))\]

This lets us formalizes the statement "more probability going to larger $w_i$" from earlier.
\begin{equation*}
\begin{aligned}
	\frac{d p_i}{d \mu} =& \left(\frac{-w_i}{Z(b)}\exp(-bw_i)-\frac{Z'(b)}{Z^2(b)}\exp(-bw_i)\right)\frac{d\,b}{d\,\mu}\\
	=&p_i(\mu-w_i)\frac{d\,b}{d\,\mu}\\
\end{aligned}
\end{equation*}

And also:
\[\frac{d\,H}{d\,b} = b\frac{d\,\mu}{d\,b}+\mu+\frac{Z'(b)}{Z(b)} = b\frac{d\,\mu}{d\,b}+\mu -\mu =b\frac{d\,\mu}{d\,b}\]
Which gives:
\[\frac{d\, H}{d\, \mu} = b\]
As expected.

\subsection{A General Case}
There's actually a lot of important probability distributions what have a similar form.
The numerator is an exponential  of a weight times a constant and the denominator is,
naturally, the sum of these exponentials.
\\

Start with a similar Lagrangian as before:
\[ L = -\sum_ip_i\ln(p_i)-a\left(\sum_i p_i - 1\right) - bF\]
Where $F$ is a function of just $p_i$.
The goal is to extremize entropy while keeping normalization and the constraint $F$,
giving:
\[p_i = \exp\left(-1-a-b\frac{\partial F}{\partial p_i}\right)\]
Defining $Z$ similar to before:
\[Z(t) = \sum_i\exp\left(-t\frac{\partial F}{\partial p_i}\right)\]
Lets us normalize out $a$ like before:
\[p_i = \frac{1}{Z(b)}\exp\left(-b\frac{\partial F}{\partial p_i}\right)\]
\\

We can also similarly define the average of the weights:
\begin{equation*}
\begin{aligned}
	\sum_ip_i\frac{\partial F}{\partial p_i} =&
	\sum_i\frac{1}{Z(b)}\exp\left(-b\frac{\partial F}{\partial p_i}\right)\frac{\partial F}{\partial p_i}\\
	=&-\frac{Z'(b)}{Z(b)}\\
	=&-\frac{d}{d\,b}\ln(Z(b))
\end{aligned}
\end{equation*}

